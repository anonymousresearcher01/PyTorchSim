{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aac974-97ea-46f2-b856-7b37c0a23add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/npu/build.ninja...\n",
      "Building extension module npu...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module npu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch._dynamo\n",
    "import torch.utils.cpp_extension\n",
    "base_dir = os.environ.get('TORCHSIM_DIR', default='/workspace/PyTorchSim')\n",
    "sys.path.append(base_dir)\n",
    "\n",
    "from Scheduler.scheduler import PyTorchSimRunner\n",
    "module = PyTorchSimRunner.setup_device()\n",
    "device = module.custom_device()\n",
    "\n",
    "def test_result(name, out, cpu_out, rtol=1e-4, atol=1e-4):\n",
    "    if torch.allclose(out.cpu(), cpu_out, rtol=rtol, atol=atol):\n",
    "        message = f\"|{name} Test Passed|\"\n",
    "        print(\"-\" * len(message))\n",
    "        print(message)\n",
    "        print(\"-\" * len(message))\n",
    "    else:\n",
    "        message = f\"|{name} Test Failed|\"\n",
    "        print(\"-\" * len(message))\n",
    "        print(message)\n",
    "        print(\"-\" * len(message))\n",
    "        print(\"custom out: \", out.cpu())\n",
    "        print(\"cpu out: \", cpu_out)\n",
    "        exit(1)\n",
    "\n",
    "def test_exponent2(device, size=(128, 128)):\n",
    "    def exponent2(a):\n",
    "        return a.exp2()\n",
    "    x = torch.randn(size).to(device=device)\n",
    "    opt_fn = torch.compile(dynamic=False)(exponent2)\n",
    "    res = opt_fn(x)\n",
    "    out = exponent2(x.cpu())\n",
    "    test_result(\"exponent2\", res, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d509f3-d955-4149-9f0f-bd0f3d0620f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-03 02:02:14,679] [0/0] torch._inductor.debug: [WARNING] model___9 debug trace: /tmp/torchinductor_root/uu/cuumxtbdv4ukzpymchmrda2exohouwcdybawmj2v7jog4vbvoycf.debug\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapper Codegen Path = /tmp/torchinductor_root/uu/cuumxtbdv4ukzpymchmrda2exohouwcdybawmj2v7jog4vbvoycf.py\n",
      "[Gem5] Gem5 is running... \n",
      "[Spike] Running Spike simulator\n",
      "[TOGSim] TOGSim is running..  \n",
      "[TOGSim] Simulation log is stored to \"/workspace/PyTorchSim/togsim_results/20251203_020218.log\"\n",
      "------------------\n",
      "|exp2 Test Passed|\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(16, 16)\n",
    "npu_x = input.to(device=device)\n",
    "cpu_x = input.to(\"cpu\")\n",
    "func = torch.exp2\n",
    "opt_fn = torch.compile(dynamic=False)(func)\n",
    "npu_out = opt_fn(npu_x)\n",
    "cpu_out = func(cpu_x)\n",
    "test_result(\"exp2\", npu_out, cpu_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bfdf22f-e749-41a5-a2cf-dcbb630bfb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from ctypes import c_void_p, c_long\n",
      "import torch\n",
      "import math\n",
      "import random\n",
      "import os\n",
      "import tempfile\n",
      "from math import inf, nan\n",
      "from torch._inductor.hooks import run_intermediate_hooks\n",
      "from torch._inductor.utils import maybe_profile\n",
      "from torch._inductor.codegen.memory_planning import _align as align\n",
      "\n",
      "from torch import device, empty, empty_strided\n",
      "from PyTorchSimFrontend.extension_codecache import CustomAsyncCompile\n",
      "from PyTorchSimFrontend.extension_config import CONFIG_SRAM_BUFFER_PLAN, CONFIG_TOGSIM_EAGER_MODE\n",
      "from Simulator.simulator import TOGSimulator\n",
      "from PyTorchSimFrontend.extension_op import sparse_mm_dummy_stonne_outer\n",
      "from torch._inductor.select_algorithm import extern_kernels\n",
      "\n",
      "aten = torch.ops.aten\n",
      "inductor_ops = torch.ops.inductor\n",
      "assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n",
      "alloc_from_pool = torch.ops.inductor._alloc_from_pool\n",
      "reinterpret_tensor = torch.ops.aten._reinterpret_tensor\n",
      "custom_async_compile = CustomAsyncCompile()\n",
      "os.environ[\"TORCHSIM_LAST_COMPILED_MODULE\"] = __file__\n",
      "\n",
      "def sram_plan_prefix(buffer_name, buffer):\n",
      "    if CONFIG_SRAM_BUFFER_PLAN and (buffer_name not in CONFIG_SRAM_BUFFER_PLAN):\n",
      "        return\n",
      "    buffer_size = buffer.untyped_storage().size()\n",
      "    start = buffer.data_ptr()\n",
      "    end = start + buffer_size\n",
      "    # print(f'Alloc {buffer_name}(0x{start:x} ~ 0x{end:x})')\n",
      "    TOGSimulator.sram_alloc(buffer_name, [start, end])\n",
      "\n",
      "def sram_plan_postfix(buffer_name, buffer):\n",
      "    if CONFIG_SRAM_BUFFER_PLAN and (buffer_name not in CONFIG_SRAM_BUFFER_PLAN):\n",
      "        return\n",
      "    buffer_size = buffer.untyped_storage().size()\n",
      "    start = buffer.data_ptr()\n",
      "    end = start + buffer_size\n",
      "    # print(f'Dealloc {buffer_name}(0x{start:x} ~ 0x{end:x})')\n",
      "    TOGSimulator.sram_dealloc(buffer_name, [start, end])\n",
      "\n",
      "def host2device_memcopy(buffer):\n",
      "    pass\n",
      "\n",
      "def device2host_memcpy(buffer):\n",
      "    pass\n",
      "\n",
      "print(f'Wrapper Codegen Path = {__file__}')\n",
      "arg_attributes = [['arg0_1', [1, torch.float32, 256, [16, 16], [16, 1]]], ['buf0', [2, torch.float32, 256, [16, 16], [16, 1]]]]\n",
      "\n",
      "\n",
      "extension_kernel_0 = custom_async_compile.mlir('''memref.global @buf0_spad : memref<256xf32, 1>\n",
      "memref.global @buf1_spad : memref<256xf32, 1>\n",
      "func.func @kernel(%in_ptr0: memref<256xf32>,\n",
      "                       %out_ptr0: memref<256xf32>)\n",
      "{\n",
      "    %const0 = arith.constant 0 : index\n",
      "    %const1 = arith.constant 2 : index\n",
      "    %const2 = arith.constant 3 : index\n",
      "    %alloc0 = memref.alloc() : memref<1xi32> // 0\n",
      "    %alloc1 = memref.alloc() : memref<1xi32> // 1\n",
      "    %spad0 = memref.get_global @buf0_spad : memref<256xf32, 1>\n",
      "    %spad1 = memref.get_global @buf1_spad : memref<256xf32, 1>\n",
      "    affine.for %index0 = 0 to 256 step 256\n",
      "    {\n",
      "        memref.dma_start %in_ptr0[%index0], %spad0[%const0], %const1, %alloc0[%const0], %const0, %const1 : memref<256xf32>, memref<256xf32, 1>, memref<1xi32> {dram_stride=[1], sram_stride=[1], padding=0}\n",
      "        affine.for %compute_idx = 0 to 2 step 2\n",
      "        {\n",
      "            %tmp0 = affine.vector_load %spad0[%compute_idx] : memref<256xf32, 1>, vector<2xf32>\n",
      "            %tmp1 = arith.constant 0.69314718055994528623 : f32\n",
      "            %tmp2 = vector.broadcast %tmp1 : f32 to vector<2xf32>\n",
      "            %tmp3 = arith.mulf %tmp0, %tmp2 : vector<2xf32>\n",
      "            %tmp4 = math.exp %tmp3 : vector<2xf32>\n",
      "            affine.vector_store %tmp4, %spad1[%compute_idx] : memref<256xf32, 1>, vector<2xf32>\n",
      "        } {inner_loop=false}\n",
      "        memref.dma_start %spad1[%const0], %out_ptr0[%index0], %const2, %alloc1[%const0], %const0, %const1 : memref<256xf32, 1>, memref<256xf32>, memref<1xi32> {dram_stride=[1], sram_stride=[1], padding=0}\n",
      "    } {outer_loop=true}\n",
      "    return\n",
      "}\n",
      "''', \n",
      "vectorlane_size=128,\n",
      "loop_size=None,\n",
      "spad_info={'spad_vaddr': 3489660928, 'spad_paddr': 137438953472, 'spad_size': 131072},\n",
      "origins={'exp2'},\n",
      "arg_attributes=arg_attributes,\n",
      "vlen=256)\n",
      "\n",
      "def call(args):\n",
      "    arg0_1, = args\n",
      "    args.clear()\n",
      "    assert_size_stride(arg0_1, (16, 16), (16, 1))\n",
      "    sram_plan_prefix('arg0_1', arg0_1)\n",
      "    buf0 = empty((16, 16), device='npu', dtype=torch.float32)\n",
      "    sram_plan_prefix('buf0', buf0)\n",
      "    extension_kernel_0(arg0_1, buf0)\n",
      "    sram_plan_postfix('arg0_1', arg0_1)\n",
      "    del arg0_1\n",
      "    sram_plan_postfix('buf0', buf0)\n",
      "    return (buf0, )\n",
      "\n",
      "\n",
      "def benchmark_compiled_module(times=10, repeat=10):\n",
      "    from torch._dynamo.testing import rand_strided\n",
      "    from torch._inductor.utils import print_performance\n",
      "    arg0_1 = rand_strided((16, 16), (16, 1), device='npu:0', dtype=torch.float32)\n",
      "    fn = lambda: call([arg0_1])\n",
      "    return print_performance(fn, times=times, repeat=repeat)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    from torch._inductor.wrapper_benchmark import compiled_module_main\n",
      "    compiled_module_main('None', benchmark_compiled_module)\n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/torchinductor_root/uu/cuumxtbdv4ukzpymchmrda2exohouwcdybawmj2v7jog4vbvoycf.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
