import os
import json
from pathlib import Path
from torch import empty_strided
from typing import List, Optional
import sympy

from PyTorchSimFrontend.mlir.mlir_template import MLIRTemplate
from PyTorchSimFrontend.mlir.mlir_template import MLIRTemplateKernel
from torch._inductor.ir import IRNode
from torch._inductor.codecache import write_atomic
import PyTorchSimFrontend.extension_codecache as extension_codecache
from PyTorchSimFrontend import extension_config
from PyTorchSimFrontend.mlir import mlir_common

GEMM_TEMPLATE = r"""
// GEMM {% if prologue_nodes -%}prologue fused{%- endif %} {% if epilogue_nodes -%}eilogue fused{%- endif %} kernel
// M = {{ M }}
// N = {{ N }}
// K = {{ K }}
// TILE_M = {{ TILE_M }}
// TILE_N = {{ TILE_N }}
// TILE_K = {{ TILE_K }}
// SUB_TILE_M = {{ SUB_TILE_M }}
// SUB_TILE_N = {{ SUB_TILE_N }}
{{kernel.def_global_vars()}}

func.func @{{ KERNEL_NAME }}{{kernel.def_kernel(inputs=[X, W, Bias], outputs=[Y], names_str="X, W, Bias, Y", input_reorder=input_reorder)}} {
  {{ kernel.def_sram_buffer("X", X_tile_desc, indent_size=2) }}
  {{ kernel.def_sram_buffer("W", W_tile_desc, indent_size=2) }}
  {{ kernel.def_sram_buffer("Y", Y_tile_desc, indent_size=2) }}
  {% if not Bias %}
  %v0 = arith.constant dense<0.0> : vector<{{ kernel.get_spad_size_per_lane(TILE_M, TILE_N) }}xf32>{% endif %}
  {{ kernel.def_local_vars(indent_size=2) }}
  affine.for %index0 = 0 to {{ M }} step {{ TILE_M }} {
    affine.for %index1 = 0 to {{ N }} step {{ TILE_N }} {
      {%- if Bias %}
      {{ kernel.def_dma_op("MVIN", "Bias", Bias_idx, Bias_tile_desc, subtile_size=[SUB_TILE_M, SUB_TILE_N], indent_size=6) }}
      {%- else %}
      affine.vector_store %v0, %Y_buffer[0, 0] : {{ Y_tile_desc.get_mlir_shape(DATA_STYPE) }}, vector<{{ kernel.get_spad_size_per_lane(TILE_M, TILE_N) }}xf32>
      {%- endif %}
      affine.for %index2 = 0 to {{ K }} step {{ TILE_K }} {
        {% if prologue_nodes -%}
        // prologue nodes
        {{kernel.load_input(indent_size=8)}}
        {%- else -%}
        {{ kernel.def_dma_op("MVIN", "X", X_idx, X_tile_desc, subtile_size=[SUB_TILE_M, SUB_TILE_K], indent_size=8) }}
        {{ kernel.def_dma_op("MVIN", "W", W_idx, W_tile_desc, subtile_size=[SUB_TILE_K, SUB_TILE_N], indent_size=8) }}
        {%- endif %}
        linalg.matmul ins(%X_buffer, %W_buffer : {{ X_tile_desc.get_mlir_shape(DATA_STYPE) }}, {{ W_tile_desc.get_mlir_shape(DATA_STYPE) }})
                outs(%Y_buffer : {{ Y_tile_desc.get_mlir_shape(DATA_STYPE) }})
      } { accumulation_loop=true, subtile_loop="k" }
      {{kernel.store_output(indent_size=6)}}
    } { outer_loop=true, subtile_loop="n"  }
  } { outer_loop=true, subtile_loop="m" }
  return
}
"""

EMPTY_TEMPLATE = r"""
func.func @{{ KERNEL_NAME }}{{kernel.def_kernel(inputs=[X, W, Bias], outputs=[Y], names_str="X, W, Bias, Y", input_reorder=input_reorder)}} {
    return
}
"""

GEMM_REDUCTION_TEMPLATE = r"""
// GEMM reduction kernel
// M = {{ M }}
// N = {{ N }}
// K = {{ K }}
// TILE_M = {{ TILE_M }}
// TILE_N = {{ TILE_N }}
// TILE_K = {{ TILE_K }}
// SUB_TILE_M = {{ SUB_TILE_M }}
// SUB_TILE_N = {{ SUB_TILE_N }}
{{kernel.def_global_vars()}}

func.func @{{ KERNEL_NAME }}{{kernel.def_kernel(inputs=[X, W, Bias], outputs=[Y], names_str="X, W, Bias, Y", input_reorder=input_reorder)}} {
  {{ kernel.def_sram_buffer("X", X_tile_desc, indent_size=2) }}
  {{ kernel.def_sram_buffer("W", W_tile_desc, indent_size=2) }}
  {{ kernel.def_sram_buffer("Y", Y_tile_desc, indent_size=2) }}
  {% if not Bias %}
  %v0 = arith.constant dense<0.0> : vector<{{ kernel.get_spad_size_per_lane(TILE_M, TILE_N) }}xf32>
  {% endif %}
  {{ kernel.def_local_vars(indent_size=2) }}
  affine.for %index1 = 0 to {{ N }} step {{ TILE_N }} {
    affine.for %index0 = 0 to {{ M }} step {{ TILE_M }} {
      %Y_bufferT = memref.reinterpret_cast %Y_buffer to offset: [0], sizes: [{{ TILE_M }}, {{ TILE_N }}], strides: [{{ TILE_N }}, 1] : {{ Y_tile_desc.get_mlir_shape(DATA_STYPE) }} to memref<{{ TILE_M }}x{{ TILE_N }}xf32, 1>
      {%- if Bias %}
      {{ kernel.def_dma_op("MVIN", "Bias", Bias_idx, Bias_tile_desc, subtile_size=[SUB_TILE_M, SUB_TILE_N], indent_size=6) }}
      {%- else %}
      affine.vector_store %v0, %Y_buffer[0, 0] : memref<{{ TILE_N }}x{{ TILE_M }}xf32, 1>, vector<{{ kernel.get_spad_size_per_lane(TILE_M, TILE_N) }}xf32>
      {%- endif %}
      affine.for %index2 = 0 to {{ K }} step {{ TILE_K }} {
        {{ kernel.def_dma_op("MVIN", "X", X_idx, X_tile_desc, subtile_size=[SUB_TILE_M, SUB_TILE_K], indent_size=8) }}
        {{ kernel.def_dma_op("MVIN", "W", W_idx, W_tile_desc, subtile_size=[SUB_TILE_K, SUB_TILE_N], indent_size=8) }}
        linalg.matmul ins(%X_buffer, %W_buffer : {{ X_tile_desc.get_mlir_shape(DATA_STYPE) }}, {{ W_tile_desc.get_mlir_shape(DATA_STYPE) }})
                outs(%Y_bufferT : memref<{{TILE_M}}x{{TILE_N}}x{{DATA_STYPE}}, 1>)
      } { accumulation_loop=true, subtile_loop="k" }
      {{kernel.store_output(indent_size=6)}}
    } { outer_loop=true, subtile_loop="m" }
    {{kernel.reduction_output(indent_size=4)}}
  } { outer_loop=true, subtile_loop="n" }
  return
}
"""

class MLIRGemmTemplate(MLIRTemplate):
    def __init__(self, input_nodes, layout, input_reorder=None):
        super().__init__("kernel", input_nodes, layout, input_reorder)

    def render(self,
               kernel: MLIRTemplateKernel,
               template_buffer_node = None,
               epilogue_nodes: Optional[List[IRNode]] = None,
               prologue_nodes: Optional[List[IRNode]] = None,
               **kwargs):
        if template_buffer_node is not None:
            self.output_node = template_buffer_node

        # Extract input arguments info
        X, W, Y = self.input_nodes[0], self.input_nodes[1], self.output_node
        X_tensor = empty_strided(X.layout.size, X.layout.stride)
        W_tensor = empty_strided(W.layout.size, W.layout.stride)
        if len(W_tensor.size()) > 2 or len(X_tensor.size()) > 2:
            raise NotImplementedError("Please report this case to us...")

        # Extract fusion info
        n_epilogue_node = len(epilogue_nodes) if epilogue_nodes is not None else 0
        n_prologue_node = len(prologue_nodes) if prologue_nodes is not None else 0
        n_extra_read = set()
        if epilogue_nodes is not None:
            for enode in epilogue_nodes:
                n_extra_read.update(enode.node.get_read_names())
            if self.output_node.name in n_extra_read:
                n_extra_read.remove(self.output_node.name)

        # Select tile size
        M, N, K = X_tensor.size()[0], W_tensor.size()[1], X_tensor.size()[1]
        TILE_M, TILE_N, TILE_K, SUB_TILE_M, SUB_TILE_N, SUB_TILE_K = self.select_tile(kernel, M, N, K, n_epilogue_node, n_extra_read, n_prologue_node)

        # Select template code
        if (M == 0) or (N == 0) or (K == 0): # exception for MoE
            template = EMPTY_TEMPLATE
            nr_rdim = 0
            epilogue_dim_aliasing = {}
        elif n_epilogue_node>=1 and epilogue_nodes[0].is_reduction():
            template = GEMM_REDUCTION_TEMPLATE
            epilogue_dim_aliasing = {"index0":"index1", "index1":"index0"}
            nr_rdim = 1
        else:
            template = GEMM_TEMPLATE
            epilogue_dim_aliasing = {"index0":"index0", "index1":"index1"}
            nr_rdim = 0

        TOG_latency = M if SUB_TILE_M > M else SUB_TILE_M
        kernel.loop_size =[TOG_latency, SUB_TILE_N, SUB_TILE_K]

        # Prepare tile descriptors
        vlane_stride = 1
        vlane_split_axis = 1
        X_tile_size = [TILE_M, TILE_K]
        X_tile_stride = [1, TILE_M]
        X_tile_desc = mlir_common.MLIRMultiDimTile(X_tile_size, kernel.vector_lane, vlane_split_axis, vlane_stride)
        X_tile_desc.set_tile_size_stride(X_tile_size, X_tile_stride)
        X_tile_desc.set_name("X_buffer")
        X_tile_desc.offset = X.get_layout().offset
        X_stride = X.get_layout().stride
        X_idx = [sympy.Symbol("index0") * X_stride[0], sympy.Symbol("index2") * X_stride[1]] # To keep index arguemnt order, we used index_list

        W_tile_size = [TILE_K, TILE_N]
        W_tile_stride = [1, TILE_K]
        W_tile_desc = mlir_common.MLIRMultiDimTile(X_tile_size, kernel.vector_lane, vlane_split_axis, vlane_stride)
        W_tile_desc.set_tile_size_stride(W_tile_size, W_tile_stride)
        W_tile_desc.set_name("W_buffer")
        W_tile_desc.offset = W.get_layout().offset
        W_stride = W.get_layout().stride
        W_idx = [sympy.Symbol("index2") * W_stride[0], sympy.Symbol("index1") * W_stride[1]]

        vlane_split_axis = vlane_split_axis if nr_rdim==0 else 0
        Y_tile_size = [TILE_M, TILE_N] if nr_rdim == 0 else [TILE_N, TILE_M]
        Y_tile_stride=[1, TILE_M] if nr_rdim == 0 else [TILE_M, 1]
        Y_tile_desc = mlir_common.MLIRMultiDimTile(Y_tile_size, kernel.vector_lane, vlane_split_axis, vlane_stride)
        Y_tile_desc.set_tile_size_stride(Y_tile_size, Y_tile_stride)
        Y_tile_desc.set_name("Y_buffer")
        Y_stride = Y.get_layout().stride
        if nr_rdim == 0:
            Y_idx = [sympy.Symbol("index0") * Y_stride[0], sympy.Symbol("index1") * Y_stride[1]]
        else:
            Y_idx = [sympy.Symbol("index1") * Y_stride[1], sympy.Symbol("index0") * Y_stride[0]]

        # Extract Bias info
        Bias = None if len(self.input_nodes) == 2 else self.input_nodes[2]
        Bias_tile_desc = mlir_common.MLIRMultiDimTile(Y_tile_size, kernel.vector_lane, vlane_split_axis, vlane_stride)
        Bias_tile_desc.set_tile_size_stride(Y_tile_size, Y_tile_stride)
        Bias_tile_desc.set_name("Y_buffer")
        if Bias is not None:
          Bias_stride = Bias.get_layout().stride
          Bias_tile_desc.offset = Bias.get_layout().offset
          if nr_rdim == 0:
            Bias_idx = [sympy.Symbol("index0") * Bias_stride[0], sympy.Symbol("index1") * Bias_stride[1]]
          else:
            Bias_idx = [sympy.Symbol("index1") * Bias_stride[1], sympy.Symbol("index0") * Bias_stride[0]]
        else:
          Bias_idx = None

        kernel.render_options = dict(
            KERNEL_NAME=self.name,
            kernel=kernel,
            M=M, N=N, K=K,
            TILE_M=TILE_M,
            TILE_N=TILE_N,
            TILE_K=TILE_K,
            SUB_TILE_M=SUB_TILE_M,
            SUB_TILE_N=SUB_TILE_N,
            SUB_TILE_K=SUB_TILE_K,
            DATA_STYPE="f32",
            X = X, W = W, Y = Y,
            Bias = Bias,
            X_idx = X_idx,
            W_idx = W_idx,
            Bias_idx = Bias_idx,
            X_tile_desc = X_tile_desc,
            W_tile_desc = W_tile_desc,
            Y_tile_desc = Y_tile_desc,
            Bias_tile_desc = Bias_tile_desc,
            epilogue_nodes = epilogue_nodes,
            prologue_nodes = prologue_nodes,
            input_reorder = self.input_reorder
        )
        if prologue_nodes:
            prologue_output_name = list(prologue_nodes[0].read_writes.writes)[0].name
            if prologue_output_name == X.get_name():
                # Input fusion case
                prologue_var = "X"
                prologue_sram_var = "X_buffer"
                prologue_tile_desc = X_tile_desc
                prologue_dim_aliasing = {"index0":"index0", "index1":"index2"}
                is_input_fused = True
            else:
                # Weight fusion case
                prologue_var = "W"
                prologue_sram_var = "W_buffer"
                prologue_tile_desc = W_tile_desc
                prologue_dim_aliasing = {"index0":"index2", "index1":"index1"}
                is_input_fused = False

            kernel.prologue_info = dict (
                input_dram_var = "X",
                input_sram_var = "X_buffer",
                input_tile_desc = X_tile_desc,
                input_idx = X_idx,
                input_subtile_size = [TILE_M, TILE_K],
                input_dim_aliasing = {"index0":"index0", "index1":"index2"},

                weight_dram_var = "W",
                weight_sram_var = "W_buffer",
                weight_tile_desc = W_tile_desc,
                weight_idx = W_idx,
                weight_subtile_size = [TILE_K, TILE_N],
                weight_dim_aliasing = {"index0":"index2", "index1":"index1"},

                # Descriptor for fusion
                dram_var = prologue_var,
                sram_var = prologue_sram_var,
                dram_tile_desc = prologue_tile_desc,
                dim_aliasing = prologue_dim_aliasing,
                is_bmm = False,
                is_input_fused = is_input_fused
            )
        kernel.epilogue_info = dict(
            output_node = self.output_node.name,
            dram_var = "Y",
            sram_var = "Y_buffer",
            dram_idx = Y_idx,
            dram_tile_desc = Y_tile_desc,
            nr_rdim = nr_rdim,
            dim_aliasing = epilogue_dim_aliasing
        )
        code = self._template_from_string(template).render(**kernel.render_options)
        kernel.add_loop_info([kernel.render_options["M"], kernel.render_options["N"], kernel.render_options["K"]], [kernel.render_options["TILE_M"], kernel.render_options["TILE_N"], kernel.render_options["TILE_K"]])
        return code

    def select_tile(self, kernel, M, N, K, n_extra_node, n_extra_read, n_prologue_node):
        # Check cheat sheet
        cheatsheet_path = extension_config.CONFIG_GEMM_CHEATSHEET_PATH
        data = {}
        if extension_config.CONFIG_GEMM_CHEATSHEET_PATH is not None:
            path = Path(cheatsheet_path)
            if path.is_file():
                with path.open("r") as f:
                    data = json.load(f)

        gemm_shape = f"{M}_{K}_{N}"
        if gemm_shape in data:
            tile_info = data[gemm_shape]
            TILE_M = tile_info["TILE_M"]
            TILE_N = tile_info["TILE_N"]
            TILE_K = tile_info["TILE_K"]
        else: # case 2: use gemm_combination_mapping
            min_tile = (n_extra_node + n_prologue_node) == 0
            TILE_M, TILE_N, TILE_K = kernel.gemm_combination_mapping(M, N, K, max(len(n_extra_read)-2, 0), n_prologue_node, min_tile=min_tile)
        # case 3: use manual tile size
        if extension_config.CONFIG_MANUAL_TILE_SIZE:
            TILE_M = extension_config.CONFIG_TILE_M
            TILE_N = extension_config.CONFIG_TILE_N
            TILE_K = extension_config.CONFIG_TILE_K

        # Edge case
        if (M == 0) or (N == 0) or (K == 0):
            TILE_M, TILE_N, TILE_K = 1, 1, 1

        # Calculate Sub Tile Size for fine-grained DMA
        if extension_config.CONFIG_SUBTILE:
            # Case 1: adjust selective fine-grained DMA (SFG-DMA)
            SUB_TILE_M = TILE_M if (TILE_M < kernel.vector_lane or n_prologue_node) else kernel.vector_lane
            if (TILE_M == M and TILE_N == N and TILE_N <= 512):
                SUB_TILE_N = TILE_N if TILE_N < kernel.vector_lane else kernel.vector_lane
            else: # Avoid Row Conflict of weights
                SUB_TILE_N = TILE_N
            SUB_TILE_K = TILE_K
            # Case 2: use manual sub tile size (FG-DMA)
            if extension_config.CONFIG_MANUAL_SUBTILE_SIZE:
                SUB_TILE_M = extension_config.CONFIG_SUBTILE_M
                SUB_TILE_N = extension_config.CONFIG_SUBTILE_N
                SUB_TILE_K = extension_config.CONFIG_SUBTILE_K
        # Case 3: None Subtile
        else:
            SUB_TILE_M = TILE_M
            SUB_TILE_N = TILE_N
            SUB_TILE_K = TILE_K
        return TILE_M,TILE_N,TILE_K, SUB_TILE_M,SUB_TILE_N,SUB_TILE_K

    def codegen_header(self, code, extra_headers):
        write_path = extension_codecache.get_write_path(code)
        if not os.path.exists(write_path):
            os.makedirs(write_path)
        spike_write_path = os.path.join(write_path, "global_var.h")
        gem5_write_path = os.path.join(write_path, "gem5_global_var.h")
        if not os.path.exists(spike_write_path):
            write_atomic(spike_write_path, extra_headers[0])
        if not os.path.exists(gem5_write_path):
            write_atomic(gem5_write_path, extra_headers[1])
